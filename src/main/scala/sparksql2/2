import scalafx.application.JFXApp
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

object SSQL2_1 extends App{
  val spark = SparkSession.builder.master("spark://pandora00:7077").getOrCreate
  import spark.implicits._
  spark.sparkContext.setLogLevel("WARN")

  val schema = StructType(Array(
    StructField("series_id", StringType),
    StructField("year", IntegerType),
    StructField("period", StringType),
    StructField("value", DoubleType),
    StructField("footnote_codes", StringType)))

  val schema2 = StructType(Array(
    StructField("series_id", StringType),
    StructField("area_type_code", StringType)))
        

  val data = spark.read.schema(schema).option("header", true).option("delimiter", "\t").csv("/data/BigData/bls/la/la.data.concatenatedStateFiles").cache()
  val series = spark.read.schema(schema2).option("header",true).option("delimiter", "\t").csv("/data/BigData/bls/la/la.series").cache()

  
  val uRate = data.filter(substring('series_id,19,2) === "03").filter('period =!= "M13")
  
  val metroIDs = series.filter('area_type_code === "B")
  val microIDs = series.filter('area_type_code === "D") 
  val countyIDs = series.filter('area_type_code === "F")
  val metroData = uRate.join(metroIDs, Seq("series_id"))
  val microData = uRate.join(microIDs, Seq("series_id"))
  val countyData = uRate.join(countyIDs, Seq("series_id"))
  
  val recessionAStartMetro = metroData.filter('year === 1990 && 'period === "M6").collect()
  val recessionAEndMetro =  metroData.filter('year === 1991 && 'period === "M3")
  val recessionAStartMicro =  microData.filter('year === 1990 && 'period === "M6")
  val recessionAEndMicro =  microData.filter('year === 1991 && 'period === "M3")
  val recessionAStartCounties =  countyData.filter('year === 1990 && 'period === "M6")
  val recessionAEndCounties =  countyData.filter('year === 1991 && 'period === "M3")

  val recessionBStartMetro = metroData.filter('year === 2001 && 'period === "M2")
  val recessionBEndMetro = metroData.filter('year === 2001 && 'period === "M11")
  val recessionBStartMicro = microData.filter('year === 2001 && 'period === "M2")
  val recessionBEndMicro = microData.filter('year === 2001 && 'period === "M11")
  val recessionBStartCounties = countyData.filter('year === 2001 && 'period === "M2")
  val recessionBEndCounties = countyData.filter('year === 2001 && 'period === "M11")

  val recessionCStartMetro = metroData.filter('year === 2007 && 'period === "M11")
  val recessionCEndMetro = metroData.filter('year === 2009 && ('period === "M6"))
  val recessionCStartMicro = microData.filter('year === 2007 && 'period === "M11")
  val recessionCEndMicro = microData.filter('year === 2009 && 'period === "M6")
  val recessionCStartCounties = countyData.filter('year === 2007 && 'period === "M11")
  val recessionCEndCounties = countyData.filter('year === 2009 && 'period === "M6")

  
/*
  def histogramGrid(): Plot = {
    val bins = 0.0 to 10.0 by 1.0
    Plot.histogramGrid(bins, Seq(
        Seq((bins.map(12 - _).init:PlotDoubleSeries) -> RedARGB, (bins.map(1 + _).init:PlotDoubleSeries) -> GreenARGB),
        Seq((bins.map(c => c*c/6).tail:PlotDoubleSeries) -> BlueARGB, (bins.map(c => 10*math.random).init:PlotDoubleSeries) -> CyanARGB)), 
        false, false, "Histogram Grid", "Values", "Counts")
  }
*/


  //val selURate = uRate.select(substring('series_id,0,18).as("ID"),'year,'period,'value )
  
  //val lForce = data.filter(substring('series_id,19,2) === "06").filter('period =!= "M13").filter('value >= 10000)
  //val selLForce = lForce.select(substring('series_id,0,18).as("ID"),'year,'period,'value.as("crap"))
  
  //val combinedSHIT = selURate.join(selLForce, Seq("ID","year","period")).sort('value.desc).cache()


  
  


 
  
  //data.filter('precip > 2).select('year * 12 + 'month).show
  //println(data.filter('tmax > 100).count)
  //data.groupBy('year).mean("tmax").orderBy("avg(tmax)").show

  // question 2
  //data.filter(substring('series_id,19,2) === "04").select(max('value)).show()

  // question 3
  //println(data.filter(substring('series_id,0,1) === "G").count)

  System.out.println("~~L~O~O~K~~~O~U~T~~~B~E~L~O~W~~~B~I~G~~~D~A~T~A~~~I~N~C~O~M~I~N~G~~")

  System.out.println(recessionAStartMetro.count)
  System.out.println(recessionAEndMetro.count)
  // question 4.a
  //data.filter(substring('series_id,19,2) === "03").filter('period =!= "M13").groupBy('period).mean("value").describe().show()

  // question 4.b
  //data.filter(substring('series_id,19,2) === "03").filter('period === "M13").describe().show()
 
  // question 1.c
  //val monthlyUnemploymentRates = data.filter(substring('series_id,19,2) === "03").filter('period =!= "M13")
  //monthlyUnemploymentRates.describe()
  //System.out.println(monthlyUnemploymentRates)
  
  


  //combinedSHIT.show

  System.out.println("~~W~H~E~W~~~T~H~A~T~~~W~A~S~~~S~O~M~E~~~B~I~G~~~D~A~T~A~~")


  // question 1.d

  //User defined function example
  //myFunc = udf(("stateToNum", (s:String) => s.filter(_ != '\'').toInt)
  //data.select(myFunc('state)).show()

/*
  user Defined Aggregation example
  typed & untyped
  object WeightedAverage extends Aggregator[TempData2,(Double,Double),Double] {
    def bufferEncoder
    daf finish(reduction: (Double,Double)):Double = r._1/r._2
    def merge(
    def reduce(b: (Double,Double), a:TempData2):(Double,Double) = {
      val (vsum,wsum = b
      (vsum + a.tmax*a.precip,wsum+a.precip)
    }
    def zero : (Double,Doube) = (0.0, 0.0)
  }
*/

  //data.createOrReplaceTempView("tdata")
  //spark.sql("select tmax from tdata").show

  spark.stop()

}
